+ MEMBER=021
+ FIX_FV3=/scratch2/NCEPDEV/land/Sanath.Kumar/UFS_UTILS_SK/util/gdas_init/../../fix
+ FIX_FV3=
+ /scratch2/NCEPDEV/stmp1/Sanath.Kumar/step3
/var/spool/slurmd/job47738690/slurm_script: line 17: /scratch2/NCEPDEV/stmp1/Sanath.Kumar/step3: Is a directory
+ FIX_ORO=/scratch2/NCEPDEV/land/Sanath.Kumar/UFS_UTILS_SK/util/gdas_init/../../fix/orog
+ FIX_ORO=
+ /scratch2/NCEPDEV/stmp1/Sanath.Kumar/sfc.C48.mx500
/var/spool/slurmd/job47738690/slurm_script: line 20: /scratch2/NCEPDEV/stmp1/Sanath.Kumar/sfc.C48.mx500: Is a directory
+ FIX_AM=/scratch2/NCEPDEV/land/Sanath.Kumar/UFS_UTILS_SK/util/gdas_init/../../fix/am
+ MEMBER=gfs
+ WORKDIR=/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs
+ '[' gfs == gdas ']'
+ '[' gfs == gfs ']'
+ CTAR=C192
+ INPUT_DATA_DIR=/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/gfs.init/input/gfs.20210323/12/atmos
+ '[' '!' -d /scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/gfs.init/input/gfs.20210323/12/atmos ']'
+ ATMFILE=gfs.t12z.atmanl.nc
+ SFCFILE=gfs.t12z.sfcanl.nc
+ rm -fr /scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs
+ mkdir -p /scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs
+ cd /scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs
+ source /scratch2/NCEPDEV/land/Sanath.Kumar/UFS_UTILS_SK/util/gdas_init/../../util/gdas_init/set_fixed_files.sh
++ '[' yes = yes ']'
++ '[' C192 == C48 ']'
++ '[' C192 == C96 ']'
++ '[' C192 == C192 ']'
++ OCNRES=050
++ ORO_DIR=C192.mx050_frac
++ ORO_NAME=oro_C192.mx050
+ ORO_NAME=oro_C48.mx500
+ cat
/var/spool/slurmd/job47738690/slurm_script: line 60: fort.41: Stale file handle
+ srun /scratch2/NCEPDEV/land/Sanath.Kumar/UFS_UTILS_SK/util/gdas_init/../../exec/chgres_cube
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/scratch2/NCEPDEV/stmp1/Sanath.Kumar/data/C48.mx500.l64/work.gfs': No such file or directory: going to /tmp instead
 - INITIALIZE ESMF
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - INITIALIZE ESMF
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            0
 - READ SETUP NAMELIST
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            1
 - READ SETUP NAMELIST
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            2
 - READ SETUP NAMELIST
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            3
 - READ SETUP NAMELIST
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            4
 - READ SETUP NAMELIST
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            5
 - READ SETUP NAMELIST
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            6
 - READ SETUP NAMELIST
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            7
 - READ SETUP NAMELIST
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            8
 - READ SETUP NAMELIST
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET            9
 - READ SETUP NAMELIST
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET           10
 - READ SETUP NAMELIST
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - CALL VMGetGlobal
 - CALL VMGet
 - NPETS IS            12
 - LOCAL PET           11
 - READ SETUP NAMELIST
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
 - FATAL ERROR: READING SETUP NAMELIST.
 - IOSTAT IS:           -1
Abort(999) on node 0 (rank 0 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 0
Abort(999) on node 1 (rank 1 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 1
Abort(999) on node 2 (rank 2 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 2
Abort(999) on node 3 (rank 3 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 3
Abort(999) on node 4 (rank 4 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 4
Abort(999) on node 5 (rank 5 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 5
Abort(999) on node 6 (rank 6 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 6
Abort(999) on node 7 (rank 7 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 7
Abort(999) on node 8 (rank 8 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 8
Abort(999) on node 9 (rank 9 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 9
Abort(999) on node 10 (rank 10 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 10
Abort(999) on node 11 (rank 11 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 999) - process 11
slurmstepd: error: *** STEP 47738690.0 ON h13c16 CANCELLED AT 2023-08-04T15:52:55 ***
srun: error: h13c16: tasks 0-11: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=47738690.0
+ rc=1
+ '[' 1 '!=' 0 ']'
+ exit 1
_______________________________________________________________
Start Epilog on node h13c16 for job 47738690 :: Fri Aug  4 15:52:56 UTC 2023
Job 47738690 (serial) finished for user Sanath.Kumar in partition hera with exit code 1:0
_______________________________________________________________
End Epilogue Fri Aug  4 15:52:56 UTC 2023
